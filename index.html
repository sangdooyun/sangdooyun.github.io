<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Sangdoo Yun</title>
  
  <meta name="author" content="Sangdoo Yun">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/my_image_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Sangdoo Yun</name>
              </p>
              <p>
                I'm a research director at <a href="https://clova.ai/en/research/publications.html">Naver AI Lab</a>,
                working on various machine learning models towards real-world applications.
              </p>

              <p>
                At Naver, I've worked for network architectures (<a href="#han2021cvpr_rex">ReXNet</a>, <a href="#heo2021iccv_pit">PiT</a>),
                training techniques (<a href="#yun2019cutmix">CutMix</a>, <a href="#yun2021relabel">ReLabel</a>, <a href="#heo2021iclr_adamp">AdamP</a>,
                <a href="#heo2019iccv_od">KD</a>),
                and robustness (<a href="#bahng2020rebias">ReBias</a>).
                I've also participated on Naver's <a href="https://clova.ai/ocr">OCR</a>
                (<em>e.g.</em>, <a href="#baek2019craft">CRAFT</a>, <a href="#baek2019STR">STR</a>, <a href="#kim2021donut">Donut</a>),
                <a href="#yoo2019extd">face recognition</a>, and LLMs (<a href="#kim2023cream">Cream</a>) products.
              </p>

              <p>
                I received my MS, and PhD in computer vision at <a href="https://ece.snu.ac.kr/en">Seoul National University</a> in 2013 and 2017, respectively, under supervision of <a href="http://pil.snu.ac.kr/about/view.do?idx=1">Prof. Jin Young Choi</a>.
                I received my BS from <a href="https://ece.snu.ac.kr/en">Seoul National University</a> in 2010.
              </p>

              <p>
                I'm also an adjunct professor at <a href="https://aiis.snu.ac.kr/bbs/board.php?bo_table=sub2_6">SNU AI Inst.</a> from Sep 2022, continuing my previous position at <a href="https://cse.snu.ac.kr/en/professor/sangdoo-yun">SNU CSE Dept</a> (Sep 2021 - Aug 2022).
              </p>

              <p style="text-align:center">
                <a href="mailto:oodgnas@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/CV_sangdoo.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=o0qtjzYAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/hellbell/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="images/my_image.png" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <ul>
                <li>                  
                  Oct 2023. I gave <a href="https://docs.google.com/presentation/d/1us5RbpRrUCq4vUbX-eNhrfgwSgLNcmCNchSQCl7GOEw/edit?usp=sharing">a talk</a> at <a href="https://sites.google.com/view/rcv2023/home">RCV workshop</a> in ICCV 2023. 
                </li>                
                <li>
                  Oct 2023. One paper is accepted at EMNLP 2023: <a href="">Cream</a>. 
                </li>                
                <li>
                  Sep 2023. Two papers are accepted at NeurIPS 2023: <a href="kim2023nrg">Relation graph</a> and  <a href="">ProPILE</a> (spotlight). 
                </li>
                <li>
                  Jul 2023. Two papers are accepted at ICCV 2023: <a href="han2023luab">Neglected free lunch</a> and <a href="park2023seit">SeiT</a>. 
                </li>
                <li>
                  May 2023. One paper is accepted at ACL 2023: <a href="ahn2023mpchat">MPChat</a>. 
                </li>
                <li>
                  Jan 2023. Two papers are accepted at ICLR 2023: <a href="#park2023vitssl">CL-vs-MIM</a> and <a href="#kim2023dynaaug">Video aug</a>. 
                </li>
              </ul>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I am interested in training <strong>robust</strong>, <strong>generalizable</strong>, and <strong>transferable</strong> 
                ML models (including vision, language, and vision-language models) for <strong>real-world</strong> applications.
              </p>
            </td>
          </tr>
        </tbody></table>

 

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


        <!-- emnlp'23 cream -->
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/kim2023cream.png" alt="kim2023cream" width="160" >
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="data/kim2023cream.pdf" id="kim2023cream">
            <papertitle>Cream: Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models</papertitle>
            </a>
            <br>

            <a href="http://geewook.kim/">Geewook Kim</a>,
            <a href="">Hodong Lee</a>,
            <a href="">Daehee Kim</a>,
            <a href="">Haeji Jung </a>,
            <a href="">Sanghee Park</a>,
            <a href="">Yoonsik Kim</a>,                        
            <strong>Sangdoo Yun</strong>,
            <a href="">Taeho Kil</a>,
            <a href="">Bado Lee</a>,
            <a href="https://scholar.google.co.kr/citations?user=iowjmTwAAAAJ&hl=en">Seunghyun Park</a>
            <br>
            <em>EMNLP</em>, 2023 
            <br>
            <a href="https://arxiv.org/abs/2305.15080">arXiv</a> /
            <a href="data/kim2023cream.txt">Bibtex</a> /
            <a href="">Code</a> /
            <a href="">Demo</a>
            <p></p>
            <p>
               After introducing <a href="#kim2021donut">Donut</a>, we build Creamüç¶ which leverages large language models (LLMs).
               To mitigate the gap between vision encoders and LMs, we propose auxiliary encoders and contrastive learning scheme. 
               Cream demonstrates robust and impressive document understanding performance. 
            </p>  
          </td>
        </tr>                 
          
        <!-- neurips'23 propile -->
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/kim2023propile.png" alt="kim2023propile" width="160" >
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="data/kim2023propile.pdf" id="kim2023propile">
            <papertitle>ProPILE: Probing Privacy Leakage in Large Language Models</papertitle>
            </a>
            <br>
            <a href="https://sites.google.com/view/siwonkim">Siwon Kim</a>,
            <strong>Sangdoo Yun</strong>,
            <a href="https://hwaranlee.github.io/">Hwaran Lee</a>,
            <a href="https://gubri.eu/">Martin Gubri</a>,
            <a href="http://data.snu.ac.kr/index.php/people/">Sungroh Yoon</a>,
            <a href="https://coallaoh.github.io/">Seong Joon Oh</a>.
            <br>
            <em>NeurIPS</em>, 2023 (Spotlight)
            <br>
            <a href="https://arxiv.org/abs/2307.01881">arXiv</a> /
            <a href="data/kim2023propile.txt">Bibtex</a> /
            <a href="https://twitter.com/parameterlab/status/1706235969488208172">tweet</a>
            <a href="">Code</a> /
            <a href="">Demo</a>
            <p></p>
            <p>
              Perhaps, Large language models (LLMs) can answer just about anything, with their hyper-scale parameters and data. 
              However, they may answer your private information (i.e., personally identifiable information (PII)), then it could be problematic. 
              With our probing tool, ProPILE, we can investigate whether the model reveals our personal information or not. 
            </p>  
          </td>
        </tr>       

        <!-- neurips'23 neural graph -->
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/kim2023nrg.png" alt="kim2023nrg" width="160" >
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="data/kim2023nrg.pdf" id="kim2023nrg">
            <papertitle>Neural Relation Graph for Identifying Problematic Data</papertitle>
            </a>
            <br>
            <a href="https://janghyun1230.github.io/">Jang-Hyun Kim</a>,
            <strong>Sangdoo Yun</strong>,
            <a href="https://mllab.snu.ac.kr/hyunoh/">Hyun Oh Song</a>.
            <br>
            <em>NeurIPS</em>, 2023
            <br>
            <a href="https://arxiv.org/abs/2301.12321">arXiv</a> /
            <a href="data/kim2023nrg.txt">Bibtex</a> /
            <a href="">Code</a>
            <p></p>
            <p>
              Problematic data (e.g., outlier data or incorrect labels) harm model performance and robustness. 
              However, identifying such problematic data in large-scale datasets is quite challenging.
              Our solution focuses on the relationship among data, particularly in the feature space. 
              By utilizing our relation graph, we can easily determine whether a data point is an outlier, has a misassigned label, or is perfectly fine.
            </p>  
          </td>
        </tr>       

        <!-- arxiv compodiff -->
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/gu2023compodiff.png" alt="gu2023compodiff" width="160" >
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="data/gu2023compodiff.pdf" id="gu2023compodiff">
            <papertitle>CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion</papertitle>
            </a>
            <br>
            <a href="https://geonm.github.io/">Geonmo Gu</a>*,
            <a href="https://sanghyukchun.github.io/home/">Sanghyuk Chun</a>*,
            <a href="https://wonjae.kim/">Wonjae Kim</a>, 
            HeeJae Jun,
            Yoohoon Kang,
            <strong>Sangdoo Yun</strong>.
            <br>
            <em>*Equal contribution</em>
            <br>
            <em>arXiv</em>, 2023
            <br>
            <a href="https://arxiv.org/abs/2303.11916">arXiv</a> /
            <a href="data/gu2023compodiff.txt">Bibtex</a> /
            <a href="https://github.com/navervision/CompoDiff">Code</a> /
            <a href="https://huggingface.co/spaces/navervision/CompoDiff-Aesthetic">Demo</a> 
            <p></p>
            <p>
              We propose a diffusion-based model, <em>CompoDiff</em>, for Composed Image Retrieval (CIR) task. 
              To train the model, we created a new dataset comprising 18 million triplets of images and associated conditions. 
              <em>CompoDiff</em> shows state-of-the-art zero-shot CIR performance. 

            </p>  
          </td>
        </tr>       
        
          


        <!-- iccv'23 neglected free lunch -->
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/han2023luab.png" alt="han2023luab" width="160" >
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="data/han2023luab.pdf" id="han2023luab">
            <papertitle>Neglected Free Lunch -- Learning Image Classifiers Using Annotation Byproducts</papertitle>
            </a>
            <br>
            <a href="https://sites.google.com/site/dyhan0920/">Dongyoon Han</a>*,
            <a href="https://sites.google.com/site/junsukchoe/">Junsuk Choe</a>*,
            <a href="https://github.com/1000ship">Dante Chun</a>,
            <a href="https://johnr0.github.io/">John Joon Young Chung</a>,
            <a href="https://minsukchang.com/">Minsuk Chang</a>,
            <strong>Sangdoo Yun</strong>,
            <a href="https://jyskwon.github.io/">Jean Y. Song</a>,
            <a href="https://coallaoh.github.io/">Seong Joon Oh</a>.
            <br>
            <em>*Equal contribution</em>
            <br>
            <em>ICCV</em>, 2023
            <br>
            <a href="https://arxiv.org/abs/2303.17595">arXiv</a> /
            <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Han_Neglected_Free_Lunch_-_Learning_Image_Classifiers_Using_Annotation_Byproducts_ICCV_2023_paper.html"> cvf </a> /
            <a href="data/han2023luab.txt">Bibtex</a> /
            <a href="https://github.com/naver-ai/NeglectedFreeLunch">Code & Dataset</a> / 
            <a href="https://youtu.be/9HEj3Km2TWo">Video</a>             
            <p></p>
            <p>
              When annotating data, annotators unintionally generate auxiliary information during the annotation task, 
              such as mouse traces, mouse clicks, time durations.
              We call them <em>annotation byproducts (AB)</em>.
              We propose the new paradigm of <em>learning using annotation byproducts (LUAB)</em> which can enhance robustness of image classifiers by aligning them with human recognition mechanisms.
            </p>  
          </td>
        </tr>       




        <!-- iccv'23 seit -->
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/park2023seit.png" alt="park2023seit" width="160" >
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="data/park2023seit.pdf" id="park2023seit">
            <papertitle>SeiT: Storage-Efficient Vision Training with Tokens Using 1% of Pixel Storage</papertitle>
            </a>
            <br>
            <a href="">Song Park</a>*,
            <a href="https://sanghyukchun.github.io/home/">Sanghyuk Chun</a>*,
            <a href="https://sites.google.com/view/byeongho-heo/home">Byeongho Heo</a>,
            <a href="https://wonjae.kim/">Wonjae Kim</a>, 
            <strong>Sangdoo Yun</strong>.
            <br>
            <em>*Equal contribution</em>
            <br>
            <em>ICCV</em>, 2023
            <br>
            <a href="https://arxiv.org/abs/2303.11114">arXiv</a> /
            <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Park_SeiT_Storage-Efficient_Vision_Training_with_Tokens_Using_1_of_Pixel_ICCV_2023_paper.html"> cvf </a> / 
            <a href="data/park2023seit.txt">Bibtex</a> /
            <a href="https://github.com/naver-ai/seit">Code</a>             
            <p></p>
            <p>
              Vision deep models are image data hungry, but image storage has become a bottleneck (e.g., LAION-5B images require 240 TB).
              We propose a storage-efficient training method, <em>SeiT</em>, that utilizes only 1% of standard pixel storage without sacrificing accuracy.
            </p>  
          </td>
        </tr>           

        <!-- ACL mpchat -->
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/ahn2023mpchat.png" alt="ahn2023mpchat" width="140" >
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="data/ahn2023mpchat.pdf" id="ahn2023mpchat">
            <papertitle>MPChat: Towards Multimodal Persona-Grounded Conversation</papertitle>
            </a>
            <br>
            <a href="https://ahnjaewoo.github.io/">Jaewoo Ahn</a>, 
            <a href="https://vision.snu.ac.kr/people/yedasong.html">Yeda Song</a>, 
            <strong>Sangdoo Yun</strong>,
            <a href="https://vision.snu.ac.kr/gunhee/">Gunhee Kim</a>.
            <br>
            <em>ACL</em>, 2023
            <br>
            <a href="https://arxiv.org/abs/2305.17388">arXiv</a> /
            <a href="data/ahn2023mpchat.txt">Bibtex</a> /
            <a href="https://vision.snu.ac.kr/projects/mpchat/">Code</a> 
            <p></p>
            <p>
              Building persona is crucial for personalized dialog sistem. 
              We explore additional vision modality beyond text-based persona.  
              To this end, we collect multimodal persona dialog dataset (<em>MPChat</em>) and demonstrate how vision modality help the conversation. 

            </p>  
          </td>
        </tr>        



        
        <!-- ICLR 2023 CL-vs-MIM -->
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/park2023vitssl.png" alt="park2023vitssl" width="160" >
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="data/park2023vitssl.pdf" id="park2023vitssl">
            <papertitle>What Do Self-Supervised Vision Transformers Learn? </papertitle>
            </a>
            <br>
            <a href="https://www.namukpark.com/">Namuk Park</a>, 
            <a href="https://wonjae.kim/">Wonjae Kim</a>, 
            <a href="https://sites.google.com/view/byeongho-heo/home">Byeongho Heo</a>,
            <a href="https://scholar.google.es/citations?user=u-9bdkwAAAAJ">Taekyung Kim</a>,
            <strong>Sangdoo Yun</strong>.
            <br>            
            <em>ICLR</em>, 2023
            <br>
            <a href="">OpenReview</a> /
            <a href="https://github.com/xxxnell/cl-vs-mim-storage/blob/main/resources/cl_vs_mim_iclr2023_poster.pdf">Poster</a> /
            <a href="https://github.com/xxxnell/cl-vs-mim-storage/blob/main/resources/cl_vs_mim_talk.pdf">Slide</a> /
            <a href="https://arxiv.org/abs/2305.00729">arXiv</a> /
            <a href="data/park2023vitssl.txt">Bibtex</a> /
            <a href="https://github.com/naver-ai/cl-vs-mim">Code</a>
            
            <p></p>
            <p>
              What are the differences between contrastive learning (CL) and masked image modeling (MIM)? 
              Our findings indicate that: (1) CL captures global patterns more effectively than MIM, 
              (2) CL learns shape-oriented features while MIM focuses on texture-oriented features, 
              and (3) CL plays a key role in later layers, whereas MIM is more concentrated on early layers.              
            </p>  
          </td>
        </tr>          




        <!-- ICLR 2023 DynaAug -->
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/kim2023dynaaug.png" alt="kim2023dynaaug" width="160" >
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="data/kim2023dynaaug.pdf" id="kim2023dynaaug">
            <papertitle>Exploring Temporally Dynamic Data Augmentation for Video Recognition</papertitle>
            </a>
            <br>
            <a href="https://taeoh-kim.github.io/">Taeoh Kim</a>,
            <a href="https://scholar.google.com/citations?user=HPd_1LMAAAAJ&hl=en">Jinhyung Kim</a>,
            <a href="https://scholar.google.co.kr/citations?user=42dUnrgAAAAJ&hl=ko">Minho Shim</a>, 
            <strong>Sangdoo Yun</strong>,
            <a href="https://scholar.google.co.kr/citations?user=QyoxwbUAAAAJ&hl=ko">Myunggu Kang</a>, 
            <a href="https://scholar.google.com/citations?user=oEKX8h0AAAAJ&hl=ko">Dongyoon Wee</a>, 
            <a href="http://mvp.yonsei.ac.kr/">Sangyoun Lee</a>.
            <br>
            <em>ICLR</em>, 2023 (Notable Top 25%)
            <br>
            <a href="https://openreview.net/pdf?id=fxjzKOdw9wb">OpenReview</a> /
            <a href="https://arxiv.org/abs/2206.15015">arXiv</a> /
            <a href="data/kim2023dynaaug.txt">Bibtex</a> 
            <p></p>
            <p>              
              We introduce <em>DynaAugment</em>, a new video data augmentation to capture the temporal dynamics in videos. 
              DynaAugment changes the magnitude of augmentation operation over time to emulate temporal dynamics found in real-world videos.
            </p>  
          </td>
        </tr>  



        <!--
        NeurIPS 2022 A Unified Analysis of Mixed Sample Data Augmentation: A Loss Function Perspective
        -->

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/park2022msda.jpg" alt="park2022msda" width="160" >
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="data/park2022msda.pdf" id="park2022msda">
            <papertitle>A Unified Analysis of Mixed Sample Data Augmentation: A Loss Function Perspective</papertitle>
            </a>
            <br>
            <a href="https://chanwoo-park-official.github.io/">Chanwoo Park*</a>,            
            <strong>Sangdoo Yun*</strong>,
            <a href="https://sanghyukchun.github.io/home/">Sanghyuk Chun</a>.
            <br>
            <em>*Equal contribution</em>
            <br>
            <em>NeurIPS</em>, 2022
            <br>
            <a href="https://openreview.net/forum?id=SLdfxFdIFeN">OpenReview</a> / 
            <a href="https://arxiv.org/abs/2208.09913">arXiv</a> /
            <a href="https://nips.cc/media/PosterPDFs/NeurIPS%202022/459a4ddcb586f24efd9395aa7662bc7c.png?t=1667049606.3860223">Poster</a> /
            <a href="data/park2022msda.txt">Bibtex</a> /
            <a href="https://github.com/naver-ai/hmix-gmix">Code</a>
            <p></p>
            <p>
              Mixed sample data augmentation (MSDA), such as <a href="https://arxiv.org/abs/1710.09412">mixup</a> and <a href="#yun2019cutmix">CutMix</a>, has become a de facto strategy, 
              but its understanding is not studied deeply yet.
              We introduce the first unified theoretical analysis for MSDAs and figure out what is the difference between mixup and CutMix. 
              Up on the analysis, we build a simple hybrid version of mixup and CutMix to leverage the advantages of mixup and CutMix. 
            </p>  
          </td>
        </tr>          


        <!--
        ECCV 2022 Donut: Document Understanding Transformer without OCR
        -->
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/kim2021donut.png" alt="kim2021donut" width="160" >
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="data/kim2021donut.pdf" id="kim2021donut">
            <papertitle>Donut &#127849;: Document Understanding Transformer without OCR</papertitle>
            </a>
            <br>
            <a href="http://geewook.kim/">Geewook Kim</a>,
            <a href="http://ailab.kaist.ac.kr/users/tghong">Teakgyu Hong</a>,
            <a href="https://github.com/moonbings">Moonbin Yim</a>,
            Jinyoung Park,
            <a href="https://scholar.google.co.kr/citations?user=UltFXK0AAAAJ&hl=en">Jinyeong Yim</a>,
            <a href="https://scholar.google.com/citations?user=M13_WdcAAAAJ&hl=en">Wonseok Hwang</a>,
            <strong>Sangdoo Yun</strong>,
            <a href="https://sites.google.com/site/dyhan0920/">Dongyoon Han</a>,
            <a href="https://scholar.google.co.kr/citations?user=iowjmTwAAAAJ&hl=en">Seunghyun Park</a>
            <br>
            <em>ECCV</em>, 2022
            <br>
            <a href="https://arxiv.org/abs/2111.15664">arXiv</a> / 
            <a href="data/kim2021donut.txt">Bibtex</a> /
            <a href="https://github.com/clovaai/donut">Code</a>
            <p></p>
            <p>
              Current visual document understanding (VDU) models heavily rely on external OCR framework (<em>e.g.</em>, <a href="#baek2019craft">text detection</a>,
              <a href="#baek2019STR">text recognition</a>).
              OCR is expensive and sometimes not available.
              We bravely remove the dependency of OCR by modeling a simple transformer architecture.
              Take our highly efficient and powerful VDU model, Donut &#127849;!

            </p>
          </td>
        </tr>



        <!--
        ICML 2022 Dataset Condensation via Efficient Synthetic-Data Parameterization
        -->

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/kim2022dataset.png" alt="kim2022dataset" width="160" >
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="data/kim2022dataset.pdf" id="kim2022dataset">
            <papertitle>Dataset Condensation via Efficient Synthetic-Data Parameterization</papertitle>
            </a>
            <br>
            <a href="https://janghyun1230.github.io/">Jang-Hyun Kim</a>,
            <a href="https://mllab.snu.ac.kr/people.html">Jinuk Kim</a>,
            <a href="https://coallaoh.github.io/">Seong Joon Oh</a>,
            <strong>Sangdoo Yun</strong>,
            <a href="https://sites.google.com/site/junsukchoe/">Hwanjun Song</a>,
            <a href="https://songhwanjun.github.io/">Joonhyun Jeong</a>,
            <a href="https://aidljwha.wordpress.com/">Jung-Woo Ha</a>,
            <a href="https://mllab.snu.ac.kr/hyunoh/">Hyun Oh Song</a>.
            <br>
            <em>ICML</em>, 2022
            <br>
            <a href="data/kim2022dataset.txt">Bibtex</a> /
            <a href="https://github.com/snu-mllab/Efficient-Dataset-Condensation">Code</a>
            <p></p>
            <p>
              Data condensation is a trick to compress training data by synthesizing them into several images. 
              The goal is to obtain higher performance with lower consumption of data storage. 
              We propose practical tricks for data condensation to bring it into more practical real-world settings (e.g., 224x224 size with ImageNet) 
              beyond previous toy-ish settings (e.g., 32x32 size with CIFARs).
            </p>
          </td>
        </tr>          


        <!--
        ICML 2022 Dataset Condensation with Contrastive Signals
        -->

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/lee2022dataset.jpg" alt="lee2022dataset" width="160" >
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="data/lee2022dataset.pdf" id="lee2022dataset">
            <papertitle>Dataset Condensation with Contrastive Signals</papertitle>
            </a>
            <br>
            <a href="https://scholar.google.com/citations?user=nS24h74AAAAJ&hl=en">Saehyung Lee</a>,
            <a href="https://sanghyukchun.github.io/home/">Sanghyuk Chun</a>,
            <a href="https://sites.google.com/view/sangwon-jung">Sangwon Jung</a>,
            <strong>Sangdoo Yun</strong>,
            <a href="http://data.snu.ac.kr/index.php/people/">Sungroh Yoon</a>.
            <br>
            <em>ICML</em>, 2022
            <br>
            <a href="data/lee2022dataset.txt">Bibtex</a> 
            <p></p>
            <p>
              Existing data condensation methods deal with class-wise gradients and ignore the inter-class information. 
              We show it would degrade performance in practical scenarios like fine-grained classification. 
              Our simple remedy is modifying the loss function to integrate contrastive signals, which shows effectiveness in several practical scenarios. 
            </p>
          </td>
        </tr>          


        <!--
        CVPR 2022 Weakly Supervised Semantic Segmentation using Out-of-Distribution Data
        -->

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/lee2022cvpr_wood.png" alt="lee2022cvpr_wood" width="160" >
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="data/lee2022cvpr_wood.pdf" id="lee2022cvpr_wood">
            <papertitle>Weakly Supervised Semantic Segmentation using Out-of-Distribution Data</papertitle>
            </a>
            <br>
            <a href="https://github.com/jbeomlee93">Jungbeom Lee</a>,
            <a href="https://coallaoh.github.io/">Seong Joon Oh</a>,
            <strong>Sangdoo Yun</strong>,
            <a href="https://sites.google.com/site/junsukchoe/">Junsuk Choe</a>,
            <a href="http://data.snu.ac.kr/index.php/people/">Eunji Kim</a>,
            <a href="http://data.snu.ac.kr/index.php/people/">Sungroh Yoon</a>.
            <br>
            <em>CVPR</em>, 2022
            <br>
            <a href="data/lee2022cvpr_wood.txt">Bibtex</a> /
            <a href="https://github.com/naver-ai/w-ood">Code</a>
            <p></p>
            <p>
            Weakly supervised semantic segmentation (WSSS) suffers from spurious correlations between foreground (e.g., train) and background (e.g., rail). 
            Our idea is to collect background images without any foreground pixels (e.g., collecting railroad images without trains).
            Then we teach the model not to see the background pixels to classify foreground class. 
            Adding small amount of background images brings large performance gain in WSSS. 
            </p>
          </td>
        </tr>          


        <!--
        CVPR 2022 The Majority Can Help The Minority: Context-rich Minority Oversampling for Long-tailed Classification
        -->

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/park2021cmo.png" alt="park2021cmo" width="160" >
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="data/park2021cmo.pdf" id="park2022cmo">
            <papertitle>The Majority Can Help The Minority: Context-rich Minority Oversampling for Long-tailed Classification</papertitle>
            </a>
            <br>

            <a href="https://www.linkedin.com/in/seulki-park-49a775147?originalSubdomain=kr">Seulki Park</a>,
            <a href="https://grayhong.github.io/">Youngkyu Hong</a>,
            <a href="https://sites.google.com/view/byeongho-heo/home">Byeongho Heo</a>,
            <strong>Sangdoo Yun</strong>,
            <a href="http://pil.snu.ac.kr/about/view.do?idx=1">Jin Young Choi</a>.

            <br>
            <em>CVPR</em>, 2022
            <br>
            <a href="data/park2021cmo.txt">Bibtex</a> / 
            <a href="https://github.com/naver-ai/cmo">Code</a>            
            <p></p>
            <p>
              Data oversampling is a simple solution for long-tailed classification, but it may exacerbate overfitting with limited context information.
              Motivated from <a href="#yun2019cutmix">CutMix</a>, we introduce a simple context-rich oversampling method.
              Interestingly, majority classes play a key role for boosting classification accuracy of minority classes!
            </p>
          </td>
        </tr>
      
        <!--
        CVPR 2022 Hypergraph-Induced Semantic Tuplet Loss for Deep Metric Learning
        -->

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/lim2022hg.jpg" alt="" width="160" >
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Lim_Hypergraph-Induced_Semantic_Tuplet_Loss_for_Deep_Metric_Learning_CVPR_2022_paper.pdf" id="lim2022hg">
            <papertitle>Hypergraph-Induced Semantic Tuplet Loss for Deep Metric Learning</papertitle>
            </a>
            <br>
            <a href="https://scholar.google.com/citations?user=6jxQqmoAAAAJ&hl=en&oi=ao">Jongin Lim</a>
            <strong>Sangdoo Yun</strong>,
            <a href="https://www.linkedin.com/in/seulki-park-49a775147?originalSubdomain=kr">Seulki Park</a>,
            <a href="http://pil.snu.ac.kr/about/view.do?idx=1">Jin Young Choi</a>.
            <br>
            <em>CVPR</em>, 2022
            <br>
            <a href="lim2022hg.txt">Bibtex</a> /
            <a href="https://github.com/ljin0429/HIST">Code</a>
            <p></p>
            <p>
              We formulate deep metric learning as a hypergraph node classification problem to capture multilateral relationship by semantic tuples beyond previous pairwise relationship-based methods. 
            </p>
          </td>
        </tr>          


        <!--
        ICLR 2022 Which shortcut cues will dnns choose? a study from the parameter-space perspective
        -->
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/luca2021shortcut.png" alt="luca2021shortcut" width="160" >
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="data/luca2021shortcut.pdf" id="luca2021shortcut">
            <papertitle>Which shortcut cues will dnns choose? a study from the parameter-space perspective</papertitle>
            </a>
            <br>
            <a href="https://lucascimeca.com/">Luca Scimeca*</a>,
            <a href="https://coallaoh.github.io/">Seong Joon Oh*</a>,
            <a href="https://sanghyukchun.github.io/home/">Sanghyuk Chun</a>,
            <a href="https://scholar.google.com/citations?user=RgIBwboAAAAJ&hl=en">Michael Poli</a>,
            <strong>Sangdoo Yun</strong>.
            <br>
            <em>*Equal contribution</em>
            <br>
            <em>ICLR</em>, 2022
            <br>
            <a href="data/luca2021shortcut.txt">Bibtex</a> /
            <a href="https://openreview.net/forum?id=qRDQi3ocgR3">OpenReview</a> 
            <p></p>
            <p>
              What causes <a href="https://arxiv.org/abs/2004.07780">shortcut learning</a> problem?
              We observe the model's behaviors when we provide equal chance of being fit to multiple cues (<em>e.g.</em>, color and shape with equal chance).
              Interestingly, the model would like to fit into a certain cue (<em>e.g.</em>, color than shape) in such even situation.
              This paper explains the reason in terms of parameter-space perspective.
            </p>
          </td>
        </tr>       


        <!--
        IEEE Access 2021 Detecting and Removing Text in the Wild
        -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cho2021text.png" alt="cho2021text" width="160" >
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/iel7/6287639/9312710/09529177.pdf">
              <papertitle>Detecting and Removing Text in the Wild</papertitle>
              </a>
              <br>
              <a href="http://pil.snu.ac.kr/member/view.do?idx=9">Junho Cho</a>,
              <strong>Sangdoo Yun</strong>,
              <a href="https://sites.google.com/site/dyhan0920/">Dongyoon Han</a>,
              <a href="https://sites.google.com/view/byeongho-heo/home">Byeongho Heo</a>,
              <a href="http://pil.snu.ac.kr/about/view.do?idx=1">Jin Young Choi</a>.
              <br>
              <em>IEEE Access</em>, 2021
              <br>
              <a href="data/cho2021text.txt">Bibtex</a>
              <p></p>
              <p>
                Unifyied text detection and text removal framework for scene text removal in the wild.
              </p>
            </td>
          </tr>


        <!--
        ICCV 2021 Rethinking spatial dimensions of vision transformers
        -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/heo2021iccv_pit.png" alt="heo2021iccv_pit" width="160" >
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/heo2021iccv_pit.pdf" id="heo2021iccv_pit">
              <papertitle>Rethinking spatial dimensions of vision transformers</papertitle>
              </a>
              <br>
              <a href="https://sites.google.com/view/byeongho-heo/home">Byeongho Heo</a>,
              <strong>Sangdoo Yun</strong>,
              <a href="https://sites.google.com/site/dyhan0920/">Dongyoon Han</a>,
              <a href="https://sanghyukchun.github.io/home/">Sanghyuk Chun</a>,
              <a href="https://sites.google.com/site/junsukchoe/">Junsuk Choe</a>,
              <a href="https://coallaoh.github.io/">Seong Joon Oh</a>.
              <br>
              <em>ICCV</em>, 2021
              <br>
              <a href="data/heo2021iccv_pit.txt">Bibtex</a> /
              <a href="https://github.com/naver-ai/pit">Code</a>
              <p></p>
              <p>
                The <a href="https://arxiv.org/abs/2010.11929">Vision transformer (ViT)</a> has become a strong design principle for vision modeling.
                Because ViT is originated from NLP's <a href="http://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf">Transformer</a>,
                it has no intermediate pooling layers, which is common in CNNs.
                We simply inject the pooling concept on ViT and introduce a new architecture PiT.
              </p>
            </td>
          </tr>

        <!--
        ICCV 2021 Normalization Matters in Weakly Supervised Object Localization
        -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/kim2021iccv_wsol.png" alt="kim2021iccv_wsol" width="160" >
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Normalization_Matters_in_Weakly_Supervised_Object_Localization_ICCV_2021_paper.pdf" id="kim2021iccv_wsolnorm">
              <papertitle>Normalization Matters in Weakly Supervised Object Localization</papertitle>
              </a>
              <br>
              <a href="https://sites.google.com/site/jeesookim0305">Jeesoo Kim</a>,
              <a href="https://sites.google.com/site/junsukchoe/">Junsuk Choe</a>,
              <strong>Sangdoo Yun</strong>,
              <a href="http://mipal.snu.ac.kr/index.php/Nojun_Kwak">Nojun Kwak</a>.
              <br>
              <em>ICCV</em>, 2021
              <br>
              <a href="data/kim2021iccv_wsol.txt">Bibtex</a> /
              <a href="https://github.com/GenDisc/IVR">Code</a>
              <p></p>
              <p>
                We investigates the effect of <a href="http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf">CAM (CVPR'16)</a> normalization on WSOL,
                and suggest a new normalization method.
              </p>
            </td>
          </tr>


          <!--
          CVPR 2021 Re-labeling ImageNet: from Single to Multi-Labels, from Global to Localized Labels
          -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/yun2021relabel.png" alt="yun2021relabel" width="160" >
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/yun2021cvpr_relabel.pdf"  id="yun2021relabel">
              <papertitle>Re-labeling ImageNet: from Single to Multi-Labels, from Global to Localized Labels</papertitle>
              </a>
              <br>
              <strong>Sangdoo Yun</strong>,

              <a href="https://coallaoh.github.io/">Seong Joon Oh</a>,
              <a href="https://sites.google.com/view/byeongho-heo/home">Byeongho Heo</a>,
              <a href="https://sites.google.com/site/dyhan0920/">Dongyoon Han</a>,
              <a href="https://sites.google.com/site/junsukchoe/">Junsuk Choe</a>,
              <a href="https://sanghyukchun.github.io/home/">Sanghyuk Chun</a>.

              <br>
              <em>CVPR</em>, 2021
              <br>
              <a href="data/yun2021cvpr_relabel.txt">Bibtex</a> /
              <a href="https://github.com/naver-ai/relabel_imagenet">Code</a> /
              <a href="https://www.youtube.com/watch?v=hc29xYQIlBE">Video</a> /
              <a href="data/yun2021cvpr_relabel_poster.pdf">Poster</a>

              <p></p>
              <p>
                <a href="https://www.image-net.org/">ImageNet</a> has lots of label noises and there have been efforts to fix them on the evaluation set
                (<em>e.g.</em> <a href="http://proceedings.mlr.press/v119/shankar20c.html">Shankar <em>et al.</em></a>, <a href="https://arxiv.org/abs/2006.07159">Bayer <em>et al.</em></a>).
                We paid our attention to the training set, whose label noises have been overlooked, and release the re-labeled ImageNet and codebase
                (published at <a href="https://github.com/naver-ai/relabel_imagenet">this repo</a>).
                The re-labeled data improves the ImageNet and downstream task accuracies.
              </p>
            </td>
          </tr>


          <!--
          CVPR 2021 Rethinking Channel Dimensions for Efficient Model Design
          -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/han2021cvpr_rex.png" alt="han2021cvpr_rex" width="160" >
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/han2021cvpr_rex.pdf" id="han2021cvpr_rex">
              <papertitle>Rethinking Channel Dimensions for Efficient Model Design</papertitle>
              </a>
              <br>
              <a href="https://sites.google.com/site/dyhan0920/">Dongyoon Han</a>,
              <strong>Sangdoo Yun</strong>,
              <a href="https://sites.google.com/view/byeongho-heo/home">Byeongho Heo</a>,
              <a href="https://yjyoo3312.github.io/">Youngjoon Yoo</a>.

              <br>
              <em>CVPR</em>, 2021
              <br>
              <a href="data/han2021cvpr_rex.txt">Bibtex</a> /
              <a href="https://github.com/clovaai/rexnet">Code</a>

              <p></p>
              <p>
                CNN architectures (<em>e.g.,</em> ResNet, MobileNet, etc.) usually follows the same feature-map down-sampling policy.
                We conjecture such design policy would harm the representation ability of intermediate layers.
                We analyze the feature-map's rank (inspired by softmax-bottleneck) and suggests a new network architecture,
                namely, Rank eXpanded Network (ReXNet).
              </p>
            </td>
          </tr>


          <!--
          ICLR 2021 AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights
          -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/heo2021iclr_adamp.png" alt="heo2021iclr_adamp" width="160" >
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/heo2021iclr_adamp.pdf" id="heo2021iclr_adamp">
              <papertitle>AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights</papertitle>
              </a>
              <br>
              <a href="https://sites.google.com/view/byeongho-heo/home">Byeongho Heo*</a>,
              <a href="https://sanghyukchun.github.io/home/">Sanghyuk Chun*</a>,
              <a href="https://coallaoh.github.io/">Seong Joon Oh</a>,
              <a href="https://sites.google.com/site/dyhan0920/">Dongyoon Han</a>,
              <a href="https://sites.google.com/site/youngjunguh">Youngjung Uh</a>,
              <strong>Sangdoo Yun</strong>,
              <a href="https://sites.google.com/site/hjw9096/">Jungwoo Ha</a>.
              <br>
              <em>*Equal contribution</em>
              <br>
              <em>ICLR</em>, 2021

              <br>
              <a href="data/heo2021iclr_adamp.txt">Bibtex</a> /
              <a href="https://github.com/clovaai/AdamP">Code</a> /
              <a href="https://clovaai.github.io/AdamP/">Project</a>

              <p></p>
              <p>
                Adding projection operation on Adam and SGD optimizer to mitigate slowdown of convergence due to rapidly increased norm.
                It leads to performance improvements across the board with easy installation (<em>pip install adamp</em>).
              </p>
            </td>
          </tr>

          <!--
          arxiv 2020 VideoMix: Rethinking Data Augmentation for Video Classification
          -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/yun2020videomix.png" alt="yun2020videomix" width="160" >
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/yun2020videomix.pdf">
              <papertitle>VideoMix: Rethinking Data Augmentation for Video Classification</papertitle>
              </a>
              <br>
              <strong>Sangdoo Yun</strong>,
              <a href="https://coallaoh.github.io/">Seong Joon Oh</a>,
              <a href="https://sites.google.com/view/byeongho-heo/home">Byeongho Heo</a>,
              <a href="https://sites.google.com/site/dyhan0920/">Dongyoon Han</a>,
              <a href="https://scholar.google.com/citations?user=HPd_1LMAAAAJ&hl=en">Jinhyung Kim</a>.
              <br>
              <em>arXiv</em>, 2020
              <br>
              <a href="data/yun2020videomix.txt">Bibtex</a>
              <p></p>
              <p>Extension of <a href="#yun2019cutmix">CutMix</a> to video recognition.
                We search for the best mixing strategy for video tasks.
              </p>
            </td>
          </tr>


          <!--
          ICML 2020 Learning De-biased Representations with Biased Representations
          -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/bahng2020rebias.png" alt="bahng2020rebias" width="160" >
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/bahng2020rebias.pdf" id="bahng2020rebias">
              <papertitle>Learning De-biased Representations with Biased Representations</papertitle>
              </a>
              <br>
              <a href="https://hjbahng.github.io/">Hyojin Bahng</a>,
              <a href="https://sanghyukchun.github.io/home/">Sanghyuk Chun</a>,
              <strong>Sangdoo Yun</strong>,
              <a href="https://sangdooyun.github.io/">Jaegul Choo</a>,
              <a href="https://coallaoh.github.io/">Seong Joon Oh</a>,
              <br>
              <em>ICML</em>, 2020
              <br>
              <a href="data/bahng2020rebias.txt">Bibtex</a> /
              <a href="https://github.com/clovaai/rebias">Code</a> /
              <a href="https://icml.cc/virtual/2020/poster/5783">ICML Virtual</a> /
              <a href="https://www.youtube.com/watch?v=lkjMxZDGubA">Youtube</a>
              <p></p>
              <p>
                Models tend to learn biased representations.
                To "de-bias" model representation, we "minus" biased representation from the target model.

              </p>
            </td>
          </tr>

          <!--
          arxiv 2019 EXTD: Extremely tiny face detector via iterative filter reuse
          -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/yoo2019extd.png" alt="yoo2019extd" width="160" >
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/yoo2019extd.pdf" id="yoo2019extd">
              <papertitle>EXTD: Extremely tiny face detector via iterative filter reuse</papertitle>
              </a>
              <br>
              <a href="https://yjyoo3312.github.io/">Youngjoon Yoo</a>,
              <a href="https://sites.google.com/site/dyhan0920/">Dongyoon Han</a>,
              <strong>Sangdoo Yun</strong>.
              <br>
              <em>arXiv</em>, 2019
              <br>
              <a href="data/yoo2019extd.txt">Bibtex</a> /
              <a href="https://github.com/clovaai/EXTD_Pytorch">Code</a>

              <p></p>
              <p>
                Face detector has multi-stage for multi-resolution, but it indeed does not require such complex feature encoding.
                We introduce an extremely tiny face detector via iterative filter reuse.
              </p>
            </td>
          </tr>

          <!--
          ICCV 2019 CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features
          -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/yun2019cutmix.png" alt="yun2019cutmix" width="160" >
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/yun2019cutmix.pdf" id="yun2019cutmix">
              <papertitle>CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features</papertitle>
              </a>
              <br>
              <strong>Sangdoo Yun</strong>,
              <a href="https://sites.google.com/site/dyhan0920/">Dongyoon Han</a>,
              <a href="https://coallaoh.github.io/">Seong Joon Oh</a>,
              <a href="https://sanghyukchun.github.io/home/">Sanghyuk Chun</a>,
              <a href="https://sites.google.com/site/junsukchoe/">Junsuk Choe</a>,
              <a href="https://yjyoo3312.github.io/">Youngjoon Yoo</a>.

              <br>
              <em>ICCV</em>, 2019 <span style="color: red; "><strong>(Oral Presentation)</strong></span>
              <br>
              <a href="data/yun2019cutmix.txt">Bibtex</a> /
              <a href="https://github.com/clovaai/CutMix-PyTorch">Code</a> /
              <a href="data/yun2019cutmix_talk.pdf">Talk</a> /
              <a href="data/yun2019cutmix_poster.pdf">Poster</a> /
              <a href="https://clova-ai.blog/2019/07/15/cutmix-regularization-strategy-to-train-strong-classifiers-with-localizable-features/">Blog</a>

              <p></p>
              <p>
                Simple cut-and-paste strategy brings significant performance boosts across tasks and datasets.
              </p>
            </td>
          </tr>

          <!--
          ICCV 2019 What Is Wrong with Scene Text Recognition Model Comparisons? Dataset and Model Analysis
          -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/baek2019STR.png" alt="baek2019STR" width="160" >
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/baek2019str.pdf" id="baek2019STR">
              <papertitle>What Is Wrong with Scene Text Recognition Model Comparisons? Dataset and Model Analysis</papertitle>
              </a>
              <br>
              <a href="http://jeonghunbaek.net/">Jeonghun Baek</a>,
              <a href="http://geewook.kim/">Geewook Kim</a>,
              <a href="https://scholar.google.com/citations?user=syo1R9UAAAAJ&hl=en">Junyeop Lee</a>,
              <a href="https://sites.google.com/view/sungraepark">Sungrae Park</a>,
              <a href="https://sites.google.com/site/dyhan0920/">Dongyoon Han</a>,
              <strong>Sangdoo Yun</strong>,
              <a href="https://coallaoh.github.io/">Seong Joon Oh</a>,
              <a href="https://github.com/hwalsuklee">Hwalsuk Lee</a>.

              <br>
              <em>ICCV</em>, 2019 <span style="color: red; "><strong>(Oral Presentation)</strong></span>
              <br>
              <a href="data/baek2019STR.txt">Bibtex</a> /
              <a href="https://github.com/clovaai/deep-text-recognition-benchmark">Code</a>

              <p></p>
              <p>
                Scene text recognition evaluation has been somewhat wrong because the model and dataset were not controlled.
                We provide unified benchmark protocol and fairly reproduced results. We also found a new architecture from those unified experiments.
              </p>
            </td>
          </tr>

          <!--
          ICCV 2019 A Comprehensive Overhaul of Feature Distillation
          -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/heo2019iccv_od.png" alt="heo2019iccv_od" width="160" >
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/heo2019overhauldistill.pdf" id="heo2019iccv_od">
              <papertitle>A Comprehensive Overhaul of Feature Distillation</papertitle>
              </a>
              <br>

              <a href="https://sites.google.com/view/byeongho-heo/home">Byeongho Heo</a>,
              <a href="https://sites.google.com/site/jeesookim0305">Jeesoo Kim</a>,
              <strong>Sangdoo Yun</strong>,
              <a href="https://sites.google.com/view/hyojin-park">Hyojin Park</a>,
              <a href="http://mipal.snu.ac.kr/index.php/Nojun_Kwak">Nojun Kwak</a>,
              <a href="http://pil.snu.ac.kr/about/view.do?idx=1">Jin Young Choi</a>.

              <br>
              <em>ICCV</em>, 2019
              <br>
              <a href="data/heo2019overhauldistill.txt">Bibtex</a> /
              <a href="https://github.com/clovaai/overhaul-distillation">Code</a>

              <p></p>
              <p>
                There are lots of options for feature distillation: loss function, distillation position, teacher/student transforms.
                We study all the possible methods and provide comprehensive overhaul for feature distillation.
                Through this, we found the best feature distillation method which even beats the teacher's accuracy.
              </p>
            </td>
          </tr>


          <!--
          ICMLw 2019 An Empirical Evaluation on Robustness and Uncertainty of Regularization Methods
          -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/chun2019robustness.png" alt="chun2019robustness" width="160" >
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/chun2019robustness.pdf">
              <papertitle>An Empirical Evaluation on Robustness and Uncertainty of Regularization Methods</papertitle>
              </a>
              <br>
              <a href="https://sanghyukchun.github.io/home/">Sanghyuk Chun</a>,
              <a href="https://coallaoh.github.io/">Seong Joon Oh</a>,
              <strong>Sangdoo Yun</strong>,
              <a href="https://sites.google.com/site/dyhan0920/">Dongyoon Han</a>,
              <a href="https://sites.google.com/site/junsukchoe/">Junsuk Choe</a>,
              <a href="https://yjyoo3312.github.io/">Youngjoon Yoo</a>.

              <br>
              <em>ICML Workshop</em>, 2019
              <br>
              <a href="data/chun2019icmlw.txt">Bibtex</a>

              <p></p>
              <p>
                We provide structured experimental results for the effectiveness of regularization methods on robustness and uncertainty benchmarks.
              </p>
            </td>
          </tr>

          <!--
          CVPR 2019 Character Region Awareness for Text Detection
          -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/baek2019craft.png" alt="baek2019craft" width="160" >
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/baek2019craft.pdf" id="baek2019craft">
              <papertitle>Character Region Awareness for Text Detection</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=5rN9zcYAAAAJ&hl=en/">Youngmin Baek</a>,
              <a href="https://scholar.google.com/citations?user=UAcfGOgAAAAJ&hl=en">Bado Lee</a>,
              <a href="https://sites.google.com/site/dyhan0920/">Dongyoon Han</a>,
              <strong>Sangdoo Yun</strong>,
              <a href="https://github.com/hwalsuklee">Hwalsuk Lee</a>.

              <br>
              <em>CVPR</em>, 2019
              <br>
              <a href="data/baek2019craft.txt">Bibtex</a> /
              <a href="https://github.com/clovaai/CRAFT-pytorch">Code</a>

              <p></p>
              <p>
                 Text detectors often fail to detect real-world scene-texts, <em>e.g.</em>, curved or long texts.
                 We propose a two-stage approach; first detect individual characters and connect them.
                 We also introduce semi-weakly-supervised training trick to boost our detector's performance.
              </p>
            </td>
          </tr>

          <!--
          AAAI 2019 Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons
          -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/heo2019aaai_ab.png" alt="heo2019aaai_ab" width="160" >
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/heo2019aaai_ab.pdf">
              <papertitle>Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons</papertitle>
              </a>
              <br>

              <a href="https://sites.google.com/view/byeongho-heo/home">Byeongho Heo</a>,
              <a href="http://vml.hanyang.ac.kr/people/minsik-lee/?doing_wp_cron=1639638686.3880200386047363281250">Minsik Lee</a>,
              <strong>Sangdoo Yun</strong>,
              <a href="http://pil.snu.ac.kr/about/view.do?idx=1">Jin Young Choi</a>.

              <br>
              <em>AAAI</em>, 2019  <span style="color: red; "><strong>(Oral Presentation)</strong></span>
              <br>
              <a href="data/heo2019aaai_ab.txt">Bibtex</a> /
              <a href="https://github.com/bhheo/AB_distillation">Code</a>

              <p></p>
              <p>
                Previous feature distillation approach (<em>e.g.</em> <a href="https://arxiv.org/abs/1412.6550">FitNet</a>) focuses on mimicking the teacher's feature values.
                Rather, our goal is to transfer the actual "activation boundary" by assigning binary labels (<em>i.e.</em> activated or not) for all the neurons.
                Our loss minimizes the binary-labels' similarity.
                It shows outperforming performance against state-of-the-art KD methods.

              </p>
            </td>
          </tr>

          <!--
          AAAI 2019 Knowledge Distillation with Adversarial Samples Supporting Decision Boundary
          -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/heo2019aaai_adv.png" alt="heo2019aaai_adv" width="160" >
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/heo2019aaai_adv.pdf">
              <papertitle>Knowledge Distillation with Adversarial Samples Supporting Decision Boundary</papertitle>
              </a>
              <br>

              <a href="https://sites.google.com/view/byeongho-heo/home">Byeongho Heo</a>,
              <a href="http://vml.hanyang.ac.kr/people/minsik-lee/?doing_wp_cron=1639638686.3880200386047363281250">Minsik Lee</a>,
              <strong>Sangdoo Yun</strong>,
              <a href="http://pil.snu.ac.kr/about/view.do?idx=1">Jin Young Choi</a>.

              <br>
              <em>AAAI</em>, 2019
              <br>
              <a href="data/heo2019aaai_adv.txt">Bibtex</a> /
              <a href="https://github.com/bhheo/BSS_distillation">Code</a>

              <p></p>
              <p>
                To find teacher network's decision boundary more precisely, we adopt adversarial attack technique.
                We show the attacked samples improve distillation performance.
              </p>
            </td>
          </tr>

          <!--
          ECCV 2018 Unsupervised Holistic Image Generation from Key Local Patches
          -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/lee2018keypatchgan.png" alt="lee2018keypatchgan" width="160" >
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/lee2018keypatchgan.pdf">
              <papertitle>Unsupervised Holistic Image Generation from Key Local Patches</papertitle>
              </a>
              <br>
              <a href="http://rllab.snu.ac.kr/people/alumni-folder/donghoon-lee/donghoon-lee">Donghoon Lee</a>,
              <strong>Sangdoo Yun</strong>,
              <a href="https://sites.google.com/view/sungjoon-choi/personal">Sungjoon Choi</a>,
              <a href="http://rllab.snu.ac.kr/people/hwiyeon-yoo/hwiyeon-yoo">Hwiyeon Yoo</a>,
              <a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>,
              <a href="http://rllab.snu.ac.kr/people/songhwai-oh">Songhwai Oh</a>

              <br>
              <em>ECCV</em>, 2018
              <br>
              <a href="data/lee2018keypatchgan.txt">Bibtex</a> /
              <a href="https://github.com/hellbell/KeyPatchGan">Code</a>

              <p></p>
              <p>
                We train a GAN model that generates a holistic image from its small parts.
              </p>
            </td>
          </tr>

          <!--
          CVPR 2018 Context-aware Deep Feature Compression for High-speed Visual Tracking
          -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/choi2018traca.png" alt="choi2018traca" width="160" >
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/choi2018traca.pdf">
              <papertitle>Context-aware Deep Feature Compression for High-speed Visual Tracking</papertitle>
              </a>
              <br>

              <a href="https://sites.google.com/site/jwchoivision/">Jongwon Choi</a>,
              <a href="https://hyungjinchang.wordpress.com/">Hyung Jin Chang</a>,
              <a href="https://www.tobiasfischer.info/">Tobias Fischer</a>,
              <strong>Sangdoo Yun</strong>,
              <a href="http://pil.snu.ac.kr/member/view.do?idx=15">Kyuewang Lee</a>,
              <a href="http://pil.snu.ac.kr/member/view.do?idx=7">Jiyeoup Jeong</a>,
              <a href="https://www.imperial.ac.uk/people/y.demiris">Yiannis Demiris</a>,
              <a href="http://pil.snu.ac.kr/about/view.do?idx=1">Jin Young Choi</a>.

              <br>
              <em>CVPR</em>, 2018
              <br>
              <a href="data/choi2018traca.txt">Bibtex</a> /
              <a href="https://sites.google.com/site/jwchoivision/home/traca">Code</a>

              <p></p>
              <p>
                Correlation-based trackers have shown promising performance using hand-crafted features (<em>e.g.,</em> HOG).
                When adopting deep features for correlation-based trackers, the bottleneck is the computing costs for CNN feature extraction.
                We propose a deep feature compression method for high-speed and high-accuracy visual tracker.
              </p>
            </td>
          </tr>

          <!--
          TNNLS 2018 Action-Driven Visual Object Tracking with Deep Reinforcement Learning
          -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/yun2017adnet.png" alt="yun2018tnnls_adnet" width="160" >
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/8306309" id="yun2018_adnet">
              <papertitle>Action-Driven Visual Object Tracking with Deep Reinforcement Learning</papertitle>
              </a>
              <br>

              <strong>Sangdoo Yun</strong>,
              <a href="https://sites.google.com/site/jwchoivision/">Jongwon Choi</a>,
              <a href="https://yjyoo3312.github.io/">Youngjoon Yoo</a>,
              <a href="https://sites.google.com/view/kiminyun/profile">Kimin Yun</a>,
              <a href="http://pil.snu.ac.kr/about/view.do?idx=1">Jin Young Choi</a>.

              <br>
              <em>TNNLS</em>, 2018
              <br>
              <a href="data/yun2018tnnls.txt">Bibtex</a> /
              <a href="https://github.com/hellbell/ADNet">Code</a>

              <p></p>
              <p>
                A journal extension of <a href="#yun2017_adnet">ADNet (CVPR'17)</a>.
              </p>
            </td>
          </tr>


          <!--
          CVPR 2017 Action-Decision Networks for Visual Tracking with Deep Reinforcement Learning
          -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/yun2017adnet.png" alt="yun2017_adnet" width="160" >
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Yun_Action-Decision_Networks_for_CVPR_2017_paper.pdf" id="yun2017_adnet">
              <papertitle>Action-Decision Networks for Visual Tracking with Deep Reinforcement Learning</papertitle>
              </a>
              <br>

              <strong>Sangdoo Yun</strong>,
              <a href="https://sites.google.com/site/jwchoivision/">Jongwon Choi</a>,
              <a href="https://yjyoo3312.github.io/">Youngjoon Yoo</a>,
              <a href="https://sites.google.com/view/kiminyun/profile">Kimin Yun</a>,
              <a href="http://pil.snu.ac.kr/about/view.do?idx=1">Jin Young Choi</a>.

              <br>
              <em>CVPR</em>, 2017  <span style="color: red; "><strong>(Spotlight Presentation)</strong></span>
              <br>
              <a href="data/yun2017adnet.txt">Bibtex</a> /
              <a href="https://github.com/hellbell/ADNet">Code</a>

              <p></p>
              <p>
                We fomulate visual tracking as a decision making process and propose a reinforcement learning method to train visual trackers.
                Our RL-based tracker shows state-of-the-art level performance and especially it shows high efficiency with semi-supervised scenario.

              </p>
            </td>
          </tr>

          <!--
          CVPR 2017 Variational Autoencoded Regression: High Dimensional Regression of Visual Data on Complex Manifold
          -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/yoo2017vae.png" alt="yoo2017vae" width="160" >
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Yoo_Variational_Autoencoded_Regression_CVPR_2017_paper.pdf">
              <papertitle>Variational Autoencoded Regression: High Dimensional Regression of Visual Data on Complex Manifold</papertitle>
              </a>
              <br>

              <a href="https://yjyoo3312.github.io/">Youngjoon Yoo</a>,
              <strong>Sangdoo Yun</strong>,
              <a href="https://hyungjinchang.wordpress.com/">Hyung Jin Chang</a>,
              <a href="https://www.imperial.ac.uk/people/y.demiris">Yiannis Demiris</a>,
              <a href="http://pil.snu.ac.kr/about/view.do?idx=1">Jin Young Choi</a>.

              <br>
              <em>CVPR</em>, 2017
              <br>
              <a href="data/yoo2017vae.txt">Bibtex</a> /
              <a href="https://sites.google.com/view/yjyoo3312/project_page_vaeregression">Code</a>

              <p></p>
              <p>
                Generating visual data from given condition (<em>e.g.</em> frame index, pose skeleton, etc.) is difficult due to the visual data's high dimensions.
                Our idea is to regress the visual data in latent space which is encoded by <a href="https://arxiv.org/abs/1312.6114">VAE</a>.
                Our method can generate high-quality visual data from frame index or pose skeletons.
              </p>
            </td>
          </tr>


          <!--
          CVPR 2017 Attentional Correlation Filter Network for Adaptive Visual Tracking
          -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/choi2017acfn.png" alt="choi2017acfn" width="160" >
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Choi_Attentional_Correlation_Filter_CVPR_2017_paper.pdf">
              <papertitle>Attentional Correlation Filter Network for Adaptive Visual Tracking</papertitle>
              </a>
              <br>

              <a href="https://sites.google.com/site/jwchoivision/">Jongwon Choi</a>,
              <a href="https://hyungjinchang.wordpress.com/">Hyung Jin Chang</a>,
              <strong>Sangdoo Yun</strong>,
              <a href="https://www.tobiasfischer.info/">Tobias Fischer</a>,
              <a href="https://www.imperial.ac.uk/people/y.demiris">Yiannis Demiris</a>,
              <a href="http://pil.snu.ac.kr/about/view.do?idx=1">Jin Young Choi</a>.

              <br>
              <em>CVPR</em>, 2017
              <br>
              <a href="data/choi2017acfn.txt">Bibtex</a> /
              <a href="https://sites.google.com/site/jwchoivision/home/acfn-1">Code</a>

              <p></p>
              <p>
                Correlation-filter-based trackers usually use pre-defined feature extractor (<em>e.g.,</em> color, edge, etc).
                Using more correlation filters with diverse feature extractors at the same time will bring higher accuracy, but it induces speed-accuracy trade-off.
                This work extends the number of correlation filters more than one hundred for maximizing accuracy.
                To deal with heavy computation, we introduce a LSTM-based attentional filter selection approach.
                Our method the state-of-the-art performance amongst real-time trackers.

              </p>
            </td>
          </tr>


          <!--
          CVPR 2017 PaletteNet: Image Recolorization with Given Color Palette
          -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cho2017palette.png" alt="cho2017palette" width="160" >
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://openaccess.thecvf.com/content_cvpr_2017_workshops/w12/papers/Cho_PaletteNet_Image_Recolorization_CVPR_2017_paper.pdf">
              <papertitle>PaletteNet: Image Recolorization with Given Color Palette</papertitle>
              </a>
              <br>

              <a href="http://pil.snu.ac.kr/member/view.do?idx=9">Junho Cho</a>,
              <strong>Sangdoo Yun</strong>,
              <a href="https://cv.snu.ac.kr/index.php/~kmlee/">Kyoung Mu Lee</a>,
              <a href="http://pil.snu.ac.kr/about/view.do?idx=1">Jin Young Choi</a>.

              <br>
              <em>CVPR Workshop</em>, 2017
              <br>
              <a href="data/cho2017palette.txt">Bibtex</a>
              <p></p>
              <p>
                We propose a image colorization method from the given palette.
              </p>
            </td>
          </tr>

          <!--
          arXiv 2017 Butterfly Effect: Bidirectional Control of Classification Performance by Small Additive Perturbation
          -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/yoo2017butterfly.png" alt="yoo2017butterfly" width="160" >
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/yoo2017butterfly.pdf">
              <papertitle>Butterfly Effect: Bidirectional Control of Classification Performance by Small Additive Perturbation</papertitle>
              </a>
              <br>

              <a href="https://yjyoo3312.github.io/">Youngjoon Yoo</a>,
              <a href="http://mipal.snu.ac.kr/index.php/SeongUk_Park">Seonguk Park</a>,
              <a href="http://mipal.snu.ac.kr/index.php/Junyoung_Choi">Junyoung Choi</a>,
              <strong>Sangdoo Yun</strong>,
              <a href="http://mipal.snu.ac.kr/index.php/Nojun_Kwak">Nojun Kwak</a>.

              <br>
              <em>arXiv</em>, 2017
              <br>
              <a href="data/yoo2017butterfly.txt">Bibtex</a>
              <p></p>
              <p>
                This paper proposes a new algorithm for controlling classification results by generating a small perturbation without changing the classifier network.
                We show that the perturbation can degrade the performance like adversarial attack, or can improve classification accuracy as well.

              </p>
            </td>
          </tr>


          <!--
          CVPR 2016 Visual Path Prediction in Complex Scenes with Crowded Moving Objects
          -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/yoo2016lda.png" alt="yoo2016lda" width="160" >
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yoo_Visual_Path_Prediction_CVPR_2016_paper.pdf">
              <papertitle>Visual Path Prediction in Complex Scenes with Crowded Moving Objects</papertitle>
              </a>
              <br>

              <a href="https://yjyoo3312.github.io/">Youngjoon Yoo</a>,
              <a href="https://sites.google.com/view/kiminyun/profile">Kimin Yun</a>,
              <strong>Sangdoo Yun</strong>,
              JongHee Hong,
              <a href="https://www.linkedin.com/in/hawook-jeong-b7472b119/">Hawook Jeong</a>,
              <a href="http://pil.snu.ac.kr/about/view.do?idx=1">Jin Young Choi</a>.
              <br>
              <em>CVPR</em>, 2016
              <br>
              <a href="data/yoo2016lda.txt">Bibtex</a>
              <p></p>
              <p>
                Learn latent Dirichlet allocation model from the trajectory of people and predict future paths of people.
              </p>
            </td>
          </tr>

          <!--
          ECCV workshop 2016 Density-aware Pedestrian Proposal Networks for Robust People Detection in Crowded Scenes
          -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/yun2016density.png" alt="yun2016density" width="160" >
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=S1N49t-uZr">
              <papertitle>Density-aware Pedestrian Proposal Networks for Robust People Detection in Crowded Scenes</papertitle>
              </a>
              <br>
              <strong>Sangdoo Yun</strong>,
              <a href="https://sites.google.com/view/kiminyun/profile">Kimin Yun</a>,
              <a href="https://sites.google.com/site/jwchoivision/">Jongwon Choi</a>,
              <a href="http://pil.snu.ac.kr/about/view.do?idx=1">Jin Young Choi</a>.
              <br>
              <em>ECCV Workshop</em>, 2016
              <br>
              <a href="data/yun2016density.txt">Bibtex</a>
              <p></p>
              <p>
                Detecting people in crowded scene by considering crowd density information.
                Our intuition is more people should be detected in crowded region.
              </p>
            </td>
          </tr>

          <!--
          WACV 2016 Voting-based 3D Object Cuboid Detection Robust to Partial Occlusion from RGB-D Images
          -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/yun2016voting.png" alt="yun2016voting" width="160" >
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/7477654">
              <papertitle>Voting-based 3D Object Cuboid Detection Robust to Partial Occlusion from RGB-D Images</papertitle>
              </a>
              <br>
              <strong>Sangdoo Yun</strong>,
              <a href="https://www.linkedin.com/in/hawook-jeong-b7472b119/">Hawook Jeong</a>,
              <a href="https://www.linkedin.com/in/soo-wan-kim-53936111b/">Soo Wan Kim</a>,
              <a href="http://pil.snu.ac.kr/about/view.do?idx=1">Jin Young Choi</a>.
              <br>
              <em>WACV</em>, 2016
              <br>
              <a href="data/yun2016voting.txt">Bibtex</a>
              <p></p>
              <p>
                Predicting holistic 3D structure from pratially occluded RGB-D images.
                The key idea is a voting mechanism. Each part of an object indicates the center of the 3D structure.
              </p>
            </td>
          </tr>

          <!--
          AVSS 2014 Visual Surveillance Briefing System: Event-based Video Retrieval and Summarization
          -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/yun2014vsb.png" alt="yun2014vsb" width="160" >
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://ieeexplore.ieee.org/document/6918669/">
              <papertitle>Visual Surveillance Briefing System: Event-based Video Retrieval and Summarization</papertitle>
              </a>
              <br>
              <strong>Sangdoo Yun</strong>,
              <a href="https://sites.google.com/view/kiminyun/profile">Kimin Yun</a>,
              <a href="https://www.linkedin.com/in/soo-wan-kim-53936111b/">Soo Wan Kim</a>,
              <a href="https://yjyoo3312.github.io/">Youngjoon Yoo</a>,
              <a href="http://pil.snu.ac.kr/member/view.do?idx=7">Jiyeoup Jeong</a>.
              <br>
              <em>AVSS</em>, 2014 <span style="color: red; "><strong>(Oral Presentation)</strong></span>
              <br>
              <a href="data/yun2014vsb.txt">Bibtex</a>
              <p></p>
              <p>
                We propose a Visual Surveillance Briefing (VSB) system which generates summarized video with important events.
              </p>
            </td>
          </tr>

          <!--
          ICPR 2014 Self-organizing Cascaded Structure of Deformable Part Models for Fast Object Detection
          -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/yun2014dpm.png" alt="yun2014dpm" width="160" >
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/6977440/">
              <papertitle>Self-organizing Cascaded Structure of Deformable Part Models for Fast Object Detection</papertitle>
              </a>
              <br>
              <strong>Sangdoo Yun</strong>,
              <a href="https://www.linkedin.com/in/hawook-jeong-b7472b119/">Hawook Jeong</a>,
              Woo-Sung Kang,
              <a href="https://sites.google.com/view/byeongho-heo/home">Byeongho Heo</a>,
              <a href="http://pil.snu.ac.kr/about/view.do?idx=1">Jin Young Choi</a>.
              <br>
              <em>ICPR</em>, 2014
              <br>
              <a href="data/yun2014dpm.txt">Bibtex</a>
              <p></p>
              <p>
                We improve the computational efficiency of deformable part model (DPM) by re-organizing the order of part filters.
                With a cascaded structure, we place more important part filter at first for early rejection.
              </p>
            </td>
          </tr>

          <!--
          IVCNZ 2012 Multiple ground plane estimation for 3D scene understanding using a monocular camera
          -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/yun2012mgp.png" alt="yun2012mgp" width="160" >
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://dl.acm.org/doi/10.1145/2425836.2425838">
              <papertitle>Multiple ground plane estimation for 3D scene understanding using a monocular camera</papertitle>
              </a>
              <br>
              <strong>Sangdoo Yun</strong>,
              <a href="https://www.linkedin.com/in/soo-wan-kim-53936111b/">Soo Wan Kim</a>,
              <a href="https://www.cs.ubc.ca/~kmyi/">Kwang Moo Yi</a>,
              <a href="https://www.linkedin.com/in/haanju-yoo/">Haan-ju Yoo</a>,
              <a href="http://pil.snu.ac.kr/about/view.do?idx=1">Jin Young Choi</a>.
              <br>
              <em>IVCNZ</em>, 2012  <span style="color: red; "><strong>(Oral Presentation)</strong></span>
              <br>
              <a href="data/yun2012mgp.txt">Bibtex</a>
              <p></p>
              <p>
                Ground plain estimation is important for 3D scene understanding.
                Usually models assume the scene has a single ground plain, but sometimes it has multiple ground planes.
                We introduce multiple ground plane estimation for more robust scene understanding.

              </p>
            </td>
          </tr>


        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Academic service</heading>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/imagenet2021neurips_workshop.jpeg" alt="imagenet2021neurips_workshop" width="160">
            </td>
            <td width="75%" valign="center">
              <a href="https://sites.google.com/view/imagenet-workshop/">
              <papertitle>Workshop on ImageNet: Past, Present, and Future.</papertitle>
              </a>
              <br>
              <a href="https://eml-unitue.de/people/zeynep-akata">Zeynep Akata</a>,
              <a href="http://lucasb.eyer.be/">Lucas Beyer</a>,
              <a href="https://sanghyukchun.github.io/home/">Sanghyuk Chun</a>,
              <a href="https://eml-unitue.de/people/almut-sophia-koepke">Almut Sophia Koepke</a>,
              <a href="https://europe.naverlabs.com/people_user/diane-larlus/">Diane Larlus</a>,
              <a href="https://coallaoh.github.io/">Seong Joon Oh</a>,
              <a href="https://europe.naverlabs.com/people_user/Rafael-Sampaio-De-Rezende/">Rafael Sampaio de Rezende</a>,
              <strong>Sangdoo Yun</strong>,
              <a href="https://sites.google.com/site/xzhai89">Xiaohua Zhai</a>.
              <br>
              <em>NeurIPS</em>, 2021
              <br>
              <a href="https://sites.google.com/view/imagenet-workshop/">Website</a> /
              <a href="https://neurips.cc/virtual/2021/workshop/21879">Virtual Page</a> /
              <a href="https://www.rsipvision.com/ComputerVisionNews-2021December/12/">Preview in CV News</a>

              <p>
                <a href="https://image-net.org/">ImageNet</a> has played an important role in CV and ML in the last decade.
                It was created to train image classifiers at first but it has become a go-to benchmark for model architecture and training techniques.
                We believe now is a good time to discuss the ImageNet and its future.
                The workshop's questions will be like:
                Did we solve ImageNet?
                What have we learned from ImageNet?
                What should the next-generation ImageNet-like dataset be?
              </p>

              <br>
            </td>
          </tr>
        </tbody></table>


        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">
              <strong>Reviewing activities</strong>
              <p></p>
              <ul>
                <li>Serve as a reviewer at CVPR, ICCV, ECCV, ICML, NeurIPS, ICLR, AAAI, etc.</li>
                <li>Outstanding reviewer awards at
                  <a href="https://cvpr2021.thecvf.com/node/184">CVPR'21</a>,
                  <a href="https://iccv2021.thecvf.com/outstanding-reviewers">ICCV'21</a>,
                  <a href="https://cvpr2022.thecvf.com/outstanding-reviewers">CVPR'22</a>.
                </li>
                <li>Serve as a meta-reviewer at AAAI'22, AAAI'23, AAAI'24</li>
                <li>Serve as an area chair at NeurIPS'23 D&B Track</li>
              </ul>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="75%" valign="center">
              <strong>Talks</strong>
              <p></p>
              <ul>
                <li><a href="https://docs.google.com/presentation/d/1us5RbpRrUCq4vUbX-eNhrfgwSgLNcmCNchSQCl7GOEw/edit?usp=sharing">Towards Strong and Robust Deep Models -‚Äì Insights from Data and Supervision</a>, RCV Workshop @ ICCV 2023</li>
                <li><a href="https://docs.google.com/presentation/d/1ByUA6OK90x6hYTa6MBVXry0XKcaFWJJwWL9LcbfjDhg/edit?usp=sharing">Towards Strong and Robust Deep Models -‚Äì Insights from Data and Supervision</a>, POSTECH, Sep, 2023</li>
                <li><a href="https://docs.google.com/presentation/d/1o3v-XlqjHxp282Tb2UXDjrxu8GgM3jYoZHaVrSeYM5E/edit?usp=sharing">Towards Strong and Robust Deep Models -‚Äì Insights from Data and Supervision</a>, UNIST, Sep, 2023</li>                
                <li><a href="https://docs.google.com/presentation/d/1j3zpPP5ZwemU9KLphCEczuO-NKW3cMXQbF2qWlm7LFI/edit?usp=sharing">How to Make Deep Models Strong and Robust</a>, Sogang Uviv, Sep, 2022</li>
                <li><a href="https://docs.google.com/presentation/d/1vCyUWKJWTqE0El-m1UPe8fc6nf0MAOV5w9P-9zIg3wo/edit?usp=sharing">How to Make Deep Models Strong and Robust</a>, Korea Uviv, Apr, 2022</li>
                <li><a href="https://docs.google.com/presentation/d/1I21EApGD05KfKCLPsnbNjWgtUA7eeyPzjvfi25ZsqaA/edit?usp=sharing">How to Make Deep Models Strong and Robust</a>, UNIST, Jun, 2021</li>
              </ul>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Template borrowed from <a href="https://jonbarron.info/">Jon Barron</a> and <a href="https://coallaoh.github.io/">Seong Joon Oh</a>.
              </p>
            </td>
          </tr>
        </tbody></table>

      </tbody>
  </table>
</body>

</html>
